{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3A.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/khanyofficial/inkers1/blob/master/3A.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "TvEVBgWS1iXp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        },
        "outputId": "82403494-2f37-4394-e4d3-11854d0d88e3"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import RMSprop\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda, Conv1D, SeparableConv2D, Dropout\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 20\n",
        "\n",
        "\n",
        "def space_to_depth_x2(l ):\n",
        "    return tf.space_to_depth(l, block_size=7)\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# Hyperparameters\n",
        "batch_size = 16\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "l = 10\n",
        "num_filter = 20\n",
        "\n",
        "# Load CIFAR10 Data\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "layer1 = Conv2D(16, (5,5), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input)\n",
        "layer1 = BatchNormalization(name='norm_1')(layer1)\n",
        "layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "\n",
        "layer1 = MaxPooling2D(pool_size=(2, 2))(layer1)\n",
        "layer1 = Conv2D(16, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(layer1)\n",
        "layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "\n",
        "skip_conn = layer1\n",
        "print (layer1.shape, skip_conn.shape)\n",
        "\n",
        "layer1 = MaxPooling2D(pool_size=(2, 2))(layer1)\n",
        "layer1 = SeparableConv2D(16, (3, 3), strides=(1, 1), padding='same', name='conv_3', use_bias=False)(layer1)\n",
        "layer1 = BatchNormalization(name='norm_3')(layer1)\n",
        "layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "layer1 = MaxPooling2D(pool_size=(2, 2))(layer1)\n",
        "layer1 = Conv2D(16, (3, 3), strides=(1, 1), padding='same', name='conv_4', use_bias=False)(layer1)\n",
        "layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "layer1 = MaxPooling2D(pool_size=(2, 2))(layer1)\n",
        "\n",
        "\n",
        "skip_conn = Conv2D(16, (3,3), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_conn)\n",
        "skip_conn = BatchNormalization(name='norm_21')(skip_conn)\n",
        "skip_conn = LeakyReLU(alpha=0.1)(skip_conn)\n",
        "skip_conn = MaxPooling2D(pool_size=(2, 2))(skip_conn)\n",
        "print (layer1.shape, skip_conn.shape)\n",
        "skip_conn = Lambda(space_to_depth_x2)(skip_conn)\n",
        "print (layer1.shape, skip_conn.shape)\n",
        "\n",
        "layer1 = concatenate([layer1, skip_conn])\n",
        "layer1 = LeakyReLU(alpha=0.1)(layer1)\n",
        "layer1 = (Flatten())(layer1)\n",
        "#layer1 = Dropout(0.2)(layer1)\n",
        "output = Dense(num_classes, activation='softmax')(layer1)\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
        "\n",
        "# Test the model\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(score)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 14, 14, 16) (?, 14, 14, 16)\n",
            "(?, 1, 1, 16) (?, 7, 7, 16)\n",
            "(?, 1, 1, 16) (?, 1, 1, 784)\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 28, 28, 16)   400         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalization)     (None, 28, 28, 16)   64          conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 28, 28, 16)   0           norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 16)   0           leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 14, 14, 16)   2304        max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 14, 14, 16)   0           conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2D)  (None, 7, 7, 16)     0           leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (SeparableConv2D)        (None, 7, 7, 16)     400         max_pooling2d_7[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalization)     (None, 7, 7, 16)     64          conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 7, 7, 16)     0           norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_21 (Conv2D)                (None, 14, 14, 16)   2304        leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2D)  (None, 3, 3, 16)     0           leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_21 (BatchNormalization)    (None, 14, 14, 16)   64          conv_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_4 (Conv2D)                 (None, 3, 3, 16)     2304        max_pooling2d_8[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 14, 14, 16)   0           norm_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 3, 3, 16)     0           conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 7, 7, 16)     0           leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 1, 1, 16)     0           leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 1, 1, 784)    0           max_pooling2d_10[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1, 1, 800)    0           max_pooling2d_9[0][0]            \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 1, 1, 800)    0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 800)          0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 10)           8010        flatten_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 15,914\n",
            "Trainable params: 15,818\n",
            "Non-trainable params: 96\n",
            "__________________________________________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 405s 7ms/step - loss: 0.1303 - acc: 0.9602 - val_loss: 0.0629 - val_acc: 0.9787\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 382s 6ms/step - loss: 0.0544 - acc: 0.9835 - val_loss: 0.0418 - val_acc: 0.9864\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 276s 5ms/step - loss: 0.0410 - acc: 0.9871 - val_loss: 0.0295 - val_acc: 0.9901\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 281s 5ms/step - loss: 0.0327 - acc: 0.9896 - val_loss: 0.0289 - val_acc: 0.9895\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 296s 5ms/step - loss: 0.0258 - acc: 0.9917 - val_loss: 0.0286 - val_acc: 0.9903\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 296s 5ms/step - loss: 0.0226 - acc: 0.9927 - val_loss: 0.0272 - val_acc: 0.9915\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 294s 5ms/step - loss: 0.0190 - acc: 0.9936 - val_loss: 0.0370 - val_acc: 0.9884\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0177 - acc: 0.9940 - val_loss: 0.0285 - val_acc: 0.9918\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 298s 5ms/step - loss: 0.0151 - acc: 0.9948 - val_loss: 0.0266 - val_acc: 0.9925\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 296s 5ms/step - loss: 0.0131 - acc: 0.9954 - val_loss: 0.0242 - val_acc: 0.9925\n",
            "10000/10000 [==============================] - 14s 1ms/step\n",
            "[0.024216671837705143, 0.9925]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}