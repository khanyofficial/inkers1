{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "u17.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/khanyofficial/inkers1/blob/master/Assignment%204A.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XbasZbnX58CX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from google.colab import files, drive\n",
        "from keras import backend as k\n",
        "from keras import backend as K\n",
        "from keras.callbacks import Callback, BaseLogger, ProgbarLogger, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, LambdaCallback, LearningRateScheduler\n",
        "from keras.datasets import cifar10\n",
        "from keras.layers import Concatenate, Dense, Dropout, Conv2D, ZeroPadding2D, MaxPooling2D, BatchNormalization, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Reshape, Lambda, Conv1D, SeparableConv2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adadelta\n",
        "from keras.utils import np_utils\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import inspect\n",
        "from keras import backend\n",
        "from keras import utils\n",
        "from keras_preprocessing import image\n",
        "import math \n",
        "  \n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batchsize = 16\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        "l = 40\n",
        "num_filter = 12\n",
        "compression = 0.8\n",
        "#dropout_rate = 0.2\n",
        "\n",
        "nb_train_samples = 50000 # 3000 training samples\n",
        "nb_valid_samples = 50000 # 100 validation samples\n",
        "num_classes = 10\n",
        "\n",
        "def load_cifar10_data(img_rows, img_cols):\n",
        "\n",
        "    # Load cifar10 training and validation sets\n",
        "    (X_train, Y_train), (X_valid, Y_valid) = cifar10.load_data()\n",
        "\n",
        "    # Resize trainging images\n",
        "    if K.image_dim_ordering() == 'th':\n",
        "        X_train = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_train[:nb_train_samples,:,:,:]])\n",
        "        X_valid = np.array([cv2.resize(img.transpose(1,2,0), (img_rows,img_cols)).transpose(2,0,1) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
        "    else:\n",
        "        X_train = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_train[:nb_train_samples,:,:,:]])\n",
        "        X_valid = np.array([cv2.resize(img, (img_rows,img_cols)) for img in X_valid[:nb_valid_samples,:,:,:]])\n",
        " \n",
        "      \n",
        "    return X_train, Y_train, X_valid, Y_valid\n",
        "  \n",
        "# Load CIFAR10 Data\n",
        "x_train1, y_train1, x_test1, y_test1 = load_cifar10_data(36,36)\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "x_train1 = x_train1[:,4:36,4:36,:]\n",
        "x_test1 = x_test1[:,4:36,4:36,:]\n",
        "x_train = np.reshape(np.append(x_train, x_train1), (100000, 32,32,3))\n",
        "x_test = np.reshape(np.append(x_test, x_test1), (20000, 32,32,3))\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /=255\n",
        "x_test /=255\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# convert to one hot encoing\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "y_train1 = keras.utils.to_categorical(y_train1, num_classes)\n",
        "y_test1 = keras.utils.to_categorical(y_test1, num_classes)\n",
        "y_train = np.reshape(np.append(y_train, y_train1), (100000, 10))\n",
        "y_test = np.reshape(np.append(y_test, y_test1), (20000, 10))\n",
        "x_train = x_train[:,4:28,4:28,:]\n",
        "x_test = x_test[:,4:28,4:28,:]\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "\n",
        "#preprocessing to add files to google drive etc , add topk accuracy etc\n",
        "LOG_DIR = './log' \n",
        "\n",
        "import functools\n",
        "topk16 = functools.partial(top_k_categorical_accuracy, k=16)\n",
        "topk32 = functools.partial(top_k_categorical_accuracy, k=32)\n",
        "topk64 = functools.partial(top_k_categorical_accuracy, k=64)\n",
        "topk16.__name__ = 'topk16'\n",
        "topk32.__name__ = 'topk32'\n",
        "topk64.__name__ = 'topk64'\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "checkpoints = []\n",
        "checkpoints.append(ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4U5oLD6orTJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"Utilities for real-time data augmentation on image data.\n",
        "\"\"\"\n",
        "print(\"overriding keras preprocessing image.py file to add the below features\")\n",
        "print(\"to apply contrast_stretching, adaptive_equalization,  histogram_equalization augmentation\")\n",
        "random_rotation = image.random_rotation\n",
        "random_shift = image.random_shift\n",
        "random_shear = image.random_shear\n",
        "random_zoom = image.random_zoom\n",
        "apply_channel_shift = image.apply_channel_shift\n",
        "random_channel_shift = image.random_channel_shift\n",
        "apply_brightness_shift = image.apply_brightness_shift\n",
        "random_brightness = image.random_brightness\n",
        "apply_affine_transform = image.apply_affine_transform\n",
        "load_img = image.load_img\n",
        "\n",
        "\n",
        "def array_to_img(x, data_format=None, scale=True, dtype=None):\n",
        "    if data_format is None:\n",
        "        data_format = backend.image_data_format()\n",
        "    if 'dtype' in inspect.getargspec(image.array_to_img).args:\n",
        "        if dtype is None:\n",
        "            dtype = backend.floatx()\n",
        "        return image.array_to_img(x,\n",
        "                                  data_format=data_format,\n",
        "                                  scale=scale,\n",
        "                                  dtype=dtype)\n",
        "    return image.array_to_img(x,\n",
        "                              data_format=data_format,\n",
        "                              scale=scale)\n",
        "\n",
        "\n",
        "def img_to_array(img, data_format=None, dtype=None):\n",
        "    if data_format is None:\n",
        "        data_format = backend.image_data_format()\n",
        "    if 'dtype' in inspect.getargspec(image.img_to_array).args:\n",
        "        if dtype is None:\n",
        "            dtype = backend.floatx()\n",
        "        return image.img_to_array(img, data_format=data_format, dtype=dtype)\n",
        "    return image.img_to_array(img, data_format=data_format)\n",
        "\n",
        "\n",
        "def save_img(path,\n",
        "             x,\n",
        "             data_format=None,\n",
        "             file_format=None,\n",
        "             scale=True, **kwargs):\n",
        "    if data_format is None:\n",
        "        data_format = backend.image_data_format()\n",
        "    return image.save_img(path,\n",
        "                          x,\n",
        "                          data_format=data_format,\n",
        "                          file_format=file_format,\n",
        "                          scale=scale, **kwargs)\n",
        "\n",
        "\n",
        "class Iterator(image.Iterator, utils.Sequence):\n",
        "   \n",
        "    pass\n",
        "\n",
        "\n",
        "class DirectoryIterator(image.DirectoryIterator, Iterator):  \n",
        "\n",
        "    def __init__(self, directory, image_data_generator,\n",
        "                 target_size=(256, 256),\n",
        "                 color_mode='rgb',\n",
        "                 classes=None,\n",
        "                 class_mode='categorical',\n",
        "                 batch_size=32,\n",
        "                 shuffle=True,\n",
        "                 seed=None,\n",
        "                 data_format=None,\n",
        "                 save_to_dir=None,\n",
        "                 save_prefix='',\n",
        "                 save_format='png',\n",
        "                 follow_links=False,\n",
        "                 subset=None,\n",
        "                 interpolation='nearest',\n",
        "                 dtype=None):\n",
        "        if data_format is None:\n",
        "            data_format = backend.image_data_format()\n",
        "        kwargs = {}\n",
        "        if 'dtype' in inspect.getargspec(\n",
        "                image.ImageDataGenerator.__init__).args:\n",
        "            if dtype is None:\n",
        "                dtype = backend.floatx()\n",
        "            kwargs['dtype'] = dtype\n",
        "        super(DirectoryIterator, self).__init__(\n",
        "            directory, image_data_generator,\n",
        "            target_size=target_size,\n",
        "            color_mode=color_mode,\n",
        "            classes=classes,\n",
        "            class_mode=class_mode,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            data_format=data_format,\n",
        "            save_to_dir=save_to_dir,\n",
        "            save_prefix=save_prefix,\n",
        "            save_format=save_format,\n",
        "            follow_links=follow_links,\n",
        "            subset=subset,\n",
        "            interpolation=interpolation,\n",
        "            **kwargs)\n",
        "\n",
        "\n",
        "class NumpyArrayIterator(image.NumpyArrayIterator, Iterator):\n",
        "\n",
        "    def __init__(self, x, y, image_data_generator,\n",
        "                 batch_size=32,\n",
        "                 shuffle=False,\n",
        "                 sample_weight=None,\n",
        "                 seed=None,\n",
        "                 data_format=None,\n",
        "                 save_to_dir=None,\n",
        "                 save_prefix='',\n",
        "                 save_format='png',\n",
        "                 subset=None,\n",
        "                 dtype=None):\n",
        "        if data_format is None:\n",
        "            data_format = backend.image_data_format()\n",
        "        kwargs = {}\n",
        "        if 'dtype' in inspect.getargspec(\n",
        "                image.NumpyArrayIterator.__init__).args:\n",
        "            if dtype is None:\n",
        "                dtype = backend.floatx()\n",
        "            kwargs['dtype'] = dtype\n",
        "        super(NumpyArrayIterator, self).__init__(\n",
        "            x, y, image_data_generator,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            sample_weight=sample_weight,\n",
        "            seed=seed,\n",
        "            data_format=data_format,\n",
        "            save_to_dir=save_to_dir,\n",
        "            save_prefix=save_prefix,\n",
        "            save_format=save_format,\n",
        "            subset=subset,\n",
        "            **kwargs)\n",
        "\n",
        "\n",
        "class ImageDataGenerator(image.ImageDataGenerator):\n",
        "    def __init__(self,\n",
        "                 featurewise_center=False,\n",
        "                 samplewise_center=False,\n",
        "                 featurewise_std_normalization=False,\n",
        "                 samplewise_std_normalization=False,\n",
        "                 zca_whitening=False,\n",
        "                 zca_epsilon=1e-6,\n",
        "                 rotation_range=0,\n",
        "                 width_shift_range=0.,\n",
        "                 height_shift_range=0.,\n",
        "                 brightness_range=None,\n",
        "                 shear_range=0.,\n",
        "                 zoom_range=0.,\n",
        "                 channel_shift_range=0.,\n",
        "                 fill_mode='nearest',\n",
        "                 cval=0.,\n",
        "                 horizontal_flip=False,\n",
        "                 vertical_flip=False,\n",
        "                 rescale=None,\n",
        "                 preprocessing_function=None,\n",
        "                 data_format=None,\n",
        "                 validation_split=0.0,\n",
        "                 dtype=None,\n",
        "                 contrast_stretching=True, \n",
        "                 adaptive_equalization=True,  \n",
        "                 histogram_equalization=True):\n",
        "        if data_format is None:\n",
        "            self.counter = 0\n",
        "            data_format = backend.image_data_format()\n",
        "            self.contrast_stretching = contrast_stretching #####\n",
        "            self.adaptive_equalization = adaptive_equalization #####\n",
        "            self.histogram_equalization = histogram_equalization #####\n",
        "        kwargs = {}\n",
        "        if 'dtype' in inspect.getargspec(\n",
        "                image.ImageDataGenerator.__init__).args:\n",
        "            if dtype is None:\n",
        "                dtype = backend.floatx()\n",
        "            kwargs['dtype'] = dtype\n",
        "        super(ImageDataGenerator, self).__init__(\n",
        "            featurewise_center=featurewise_center,\n",
        "            samplewise_center=samplewise_center,\n",
        "            featurewise_std_normalization=featurewise_std_normalization,\n",
        "            samplewise_std_normalization=samplewise_std_normalization,\n",
        "            zca_whitening=zca_whitening,\n",
        "            zca_epsilon=zca_epsilon,\n",
        "            rotation_range=rotation_range,\n",
        "            width_shift_range=width_shift_range,\n",
        "            height_shift_range=height_shift_range,\n",
        "            brightness_range=brightness_range,\n",
        "            shear_range=shear_range,\n",
        "            zoom_range=zoom_range,\n",
        "            channel_shift_range=channel_shift_range,\n",
        "            fill_mode=fill_mode,\n",
        "            cval=cval,\n",
        "            horizontal_flip=horizontal_flip,\n",
        "            vertical_flip=vertical_flip,\n",
        "            rescale=rescale,\n",
        "            preprocessing_function=preprocessing_function,\n",
        "            data_format=data_format,\n",
        "            validation_split=validation_split,\n",
        "            **kwargs)\n",
        "\n",
        "\n",
        "array_to_img.__doc__ = image.array_to_img.__doc__\n",
        "img_to_array.__doc__ = image.img_to_array.__doc__\n",
        "save_img.__doc__ = image.save_img.__doc__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6fr8U_6rU6P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"helper funtion to apply contrast_stretching, adaptive_equalization,  histogram_equalization augmentation\")\n",
        "\n",
        "def random_transform(self, x):\n",
        "  img_row_axis = self.row_axis - 1\n",
        "  img_col_axis = self.col_axis - 1\n",
        "  img_channel_axis = self.channel_axis - 1\n",
        "# use composition of homographies\n",
        "# to generate final transform that needs to be applied\n",
        "  if self.rotation_range:\n",
        "     theta = np.pi / 180 * np.random.uniform(-self.rotation_range, self.rotation_range)\n",
        "  else:\n",
        "     theta = 0\n",
        "  if self.height_shift_range:\n",
        "     tx = np.random.uniform(-self.height_shift_range, self.height_shift_range) * x.shape[img_row_axis]\n",
        "  else:\n",
        "     tx = 0\n",
        "  if self.width_shift_range:\n",
        "     ty = np.random.uniform(-self.width_shift_range, self.width_shift_range) * x.shape[img_col_axis]\n",
        "  else:\n",
        "     ty = 0\n",
        "  if self.shear_range:\n",
        "     shear = np.random.uniform(-self.shear_range, self.shear_range)\n",
        "  else:\n",
        "     shear = 0\n",
        "  if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\n",
        "     zx, zy = 1, 1\n",
        "  else:\n",
        "     zx, zy = np.random.uniform(self.zoom_range[0], self.zoom_range[1], 2)\n",
        "  transform_matrix = None\n",
        "  if theta != 0:\n",
        "      rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
        "                                        [np.sin(theta), np.cos(theta), 0],\n",
        "                                        [0, 0, 1]])\n",
        "      transform_matrix = rotation_matrix\n",
        "  if tx != 0 or ty != 0:\n",
        "    shift_matrix = np.array([[1, 0, tx],\n",
        "                                     [0, 1, ty],\n",
        "                                     [0, 0, 1]])\n",
        "    transform_matrix = shift_matrix if transform_matrix is None else np.dot(transform_matrix, shift_matrix)\n",
        "  if shear != 0:\n",
        "     shear_matrix = np.array([[1, -np.sin(shear), 0],\n",
        "                                    [0, np.cos(shear), 0],\n",
        "                                    [0, 0, 1]])\n",
        "     transform_matrix = shear_matrix if transform_matrix is None else np.dot(transform_matrix, shear_matrix)\n",
        "  if zx != 1 or zy != 1:\n",
        "     zoom_matrix = np.array([[zx, 0, 0],\n",
        "                                    [0, zy, 0],\n",
        "                                    [0, 0, 1]])\n",
        "     transform_matrix = zoom_matrix if transform_matrix is None else np.dot(transform_matrix, zoom_matrix)\n",
        "  if transform_matrix is not None:\n",
        "     h, w = x.shape[img_row_axis], x.shape[img_col_axis]\n",
        "     transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)\n",
        "     x = apply_transform(x, transform_matrix, img_channel_axis,\n",
        "                                fill_mode=self.fill_mode, cval=self.cval)\n",
        "  if self.channel_shift_range != 0:\n",
        "     x = random_channel_shift(x, self.channel_shift_range, img_channel_axis)\n",
        "  if self.horizontal_flip:\n",
        "     if np.random.random() < 0.5:\n",
        "        x = flip_axis(x, img_col_axis)\n",
        "  if self.vertical_flip:\n",
        "     if np.random.random() < 0.5:\n",
        "        x = flip_axis(x, img_row_axis)\n",
        "                \n",
        "  if self.contrast_stretching: #####\n",
        "     if np.random.random() < 0.5: #####\n",
        "        p2, p98 = np.percentile(x, (2, 98)) #####\n",
        "        x = exposure.rescale_intensity(x, in_range=(p2, p98)) #####\n",
        " \n",
        "  if self.adaptive_equalization: #####\n",
        "     if np.random.random() < 0.5: #####\n",
        "        x = exposure.equalize_adapthist(x, clip_limit=0.03) #####\n",
        "                \n",
        "  if self.histogram_equalization: #####\n",
        "     if np.random.random() < 0.5: #####\n",
        "        x = exposure.equalize_hist(x) #####\n",
        "                \n",
        "  return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mQ-PElMA5-wi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "      inter_channel = num_filter * 4 \n",
        "      #temp = ZeroPadding2D((3, 3))(temp)\n",
        "      BatchNorm = BatchNormalization(epsilon=1.1e-5)(temp)\n",
        "      relu = Activation('relu')(BatchNorm)\n",
        "      Cnv1D = SeparableConv2D(inter_channel, 1, 1,  bias=False)(relu)\n",
        "      if dropout_rate>0:\n",
        "        Cnv1D = Dropout(dropout_rate)(Cnv1D)\n",
        "      \n",
        "      BN2 = BatchNormalization(epsilon=1.1e-5)(Cnv1D)\n",
        "      relu2 = Activation('relu')(BN2)\n",
        "      #zp2 = ZeroPadding2D((1, 1), name='x2_zeropadding')(relu2)\n",
        "      Conv2D_3_3 = SeparableConv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu2)\n",
        "      #zp3 = ZeroPadding2D((1, 1), name='x2_zeropadding')(Conv2D_3_3)\n",
        "      concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "      temp = concat\n",
        "      \n",
        "        \n",
        "    return temp\n",
        "\n",
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = SeparableConv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg\n",
        "\n",
        "\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBVJxy2d6DcB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_filter = 32\n",
        "dropout_rate = 0.2\n",
        "l = 10\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "x = SeparableConv2D(32, (7,7), use_bias=False ,padding='same', strides=(2, 2))(input)\n",
        "x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "x = SeparableConv2D(32, 3, 3, subsample=(2, 2), bias=False)(input)\n",
        "x = BatchNormalization(epsilon=1.1e-5)(x)\n",
        "First_Conv2D = Activation('relu')(x)\n",
        "First_Conv2D = ZeroPadding2D((1, 1))(First_Conv2D)\n",
        "First_Conv2D = MaxPooling2D((3, 3), strides=(2, 2))(First_Conv2D)\n",
        "\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = SeparableConv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)\n",
        "\n",
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxzRlreEs7xZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen  = ImageDataGenerator(rotation_range=2.5, width_shift_range=5./32, height_shift_range=5./32, horizontal_flip=True, \n",
        "                              contrast_stretching=True, adaptive_equalization=True,  histogram_equalization=True)\n",
        "\n",
        "datagen.fit(x_train)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_fT8nU21_ktm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 1-5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.2\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()\n",
        "                    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAVnxHr34VVm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 6-10\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.2\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-RzNkTpwGN7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 11-15\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.15\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mxjoww4e74WV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 16-20\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.15\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xln7xxMYuGmw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 21-25\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.1\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tTGFhHtNuMue",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 26-30\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.1\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mV8prx0yuZEZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 31-35\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.075\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M-fGDWmsuZSd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 36-40\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.075\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uBwUEzxwuZhF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 41-45\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.05\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jo4i_I5xuZu3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 46-50\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.05\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 3 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zXgHSKAyuaNG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 51-100\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.025\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 2 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-mcbUSQu9Zx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"epoch 100-150\")\n",
        "\n",
        "models.load_model(\"/content/drive/My Drive/savedfile.h5\")\n",
        "\n",
        "def step_decay(epoch):\n",
        "  initial_lrate = 0.0125\n",
        "  drop = 0.5\n",
        "  epochs_drop = 10\n",
        "  lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "  return lrate\n",
        "\n",
        "lrs = LearningRateScheduler(step_decay, verbose=1)\n",
        "checkpoints.append(lrs)\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "sgd = SGD(lr=0.2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# determine Loss function and Optimizer\n",
        "#model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test), callbacks=checkpoints)\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch= 2 * len(x_train) / 32, epochs=epochs , validation_data=(x_test, y_test), verbose = 1, callbacks=checkpoints)\n",
        "\n",
        "model.save(\"/content/drive/My Drive/savedfile.h5\")\n",
        "files.download(\"/content/drive/My Drive/savedfile.h5\")\n",
        "checkpoints.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ek-s4qhu6O6A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(filepath)\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}